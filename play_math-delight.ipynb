{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT on addition\n",
    "\n",
    "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
    "    as a sequence of integers.\n",
    "    \n",
    "    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
    "    encoding will simply be the n-digit first number, n-digit second number, \n",
    "    and (n+1)-digit result, all simply concatenated together. Because each addition\n",
    "    problem is so structured, there is no need to bother the model with encoding\n",
    "    +, =, or other tokens. Each possible sequence has the same length, and simply\n",
    "    contains the raw digits of the addition problem.\n",
    "    \n",
    "    As a few examples, the 2-digit problems:\n",
    "    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
    "    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
    "    etc.\n",
    "    \n",
    "    We will also only train GPT on the final (n+1)-digits because the first\n",
    "    two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
    "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
    "    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n",
    "    in 3 sequential steps.\n",
    "    \n",
    "    fun exercise: does it help if the result is asked to be produced in reverse order?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndigit, split):\n",
    "        self.split = split # train/test\n",
    "        self.ndigit = ndigit\n",
    "        self.vocab_size = 10 # 10 possible digits 0..9\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset for e.g. 2-digit addition\n",
    "ndigit = 2\n",
    "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
    "test_dataset = AdditionDataset(ndigit=ndigit, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0] # sample a training instance just to see what one raw example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2020 03:43:06 - INFO - mingpt.model_delight -   number of parameters: 4.393480e+05\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model_delight import DelightConfig, DelightModel\n",
    "\n",
    "# initialize a baby GPT model\n",
    "mconf = DelightConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=4, n_head=1, n_embd=128)\n",
    "model = DelightModel(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 17: train loss 1.80311. lr 5.994512e-04: 100%|██████████| 18/18 [00:25<00:00,  1.42s/it]\n",
      "08/22/2020 03:43:33 - INFO - mingpt.trainer -   test loss: 1.779751\n",
      "epoch 2 iter 17: train loss 1.59130. lr 5.977197e-04: 100%|██████████| 18/18 [00:33<00:00,  1.84s/it]\n",
      "08/22/2020 03:44:07 - INFO - mingpt.trainer -   test loss: 1.544506\n",
      "epoch 3 iter 17: train loss 1.47722. lr 5.948114e-04: 100%|██████████| 18/18 [00:26<00:00,  1.50s/it]\n",
      "08/22/2020 03:44:35 - INFO - mingpt.trainer -   test loss: 1.416624\n",
      "epoch 4 iter 17: train loss 1.38523. lr 5.907379e-04: 100%|██████████| 18/18 [00:23<00:00,  1.29s/it]\n",
      "08/22/2020 03:45:00 - INFO - mingpt.trainer -   test loss: 1.298190\n",
      "epoch 5 iter 17: train loss 1.25611. lr 5.855153e-04: 100%|██████████| 18/18 [00:30<00:00,  1.69s/it]\n",
      "08/22/2020 03:45:31 - INFO - mingpt.trainer -   test loss: 1.187733\n",
      "epoch 6 iter 17: train loss 1.24354. lr 5.791641e-04: 100%|██████████| 18/18 [00:24<00:00,  1.35s/it]\n",
      "08/22/2020 03:45:57 - INFO - mingpt.trainer -   test loss: 1.160192\n",
      "epoch 7 iter 17: train loss 1.16599. lr 5.717095e-04: 100%|██████████| 18/18 [00:28<00:00,  1.58s/it]\n",
      "08/22/2020 03:46:26 - INFO - mingpt.trainer -   test loss: 1.075767\n",
      "epoch 8 iter 17: train loss 1.13185. lr 5.631810e-04: 100%|██████████| 18/18 [00:24<00:00,  1.38s/it]\n",
      "08/22/2020 03:46:52 - INFO - mingpt.trainer -   test loss: 1.044469\n",
      "epoch 9 iter 17: train loss 1.09392. lr 5.536122e-04: 100%|██████████| 18/18 [00:19<00:00,  1.06s/it]\n",
      "08/22/2020 03:47:12 - INFO - mingpt.trainer -   test loss: 1.027030\n",
      "epoch 10 iter 17: train loss 1.07201. lr 5.430411e-04: 100%|██████████| 18/18 [00:18<00:00,  1.04s/it]\n",
      "08/22/2020 03:47:32 - INFO - mingpt.trainer -   test loss: 1.014763\n",
      "epoch 11 iter 17: train loss 1.08621. lr 5.315093e-04: 100%|██████████| 18/18 [00:16<00:00,  1.12it/s]\n",
      "08/22/2020 03:47:49 - INFO - mingpt.trainer -   test loss: 0.981222\n",
      "epoch 12 iter 17: train loss 1.04097. lr 5.190624e-04: 100%|██████████| 18/18 [00:19<00:00,  1.09s/it]\n",
      "08/22/2020 03:48:09 - INFO - mingpt.trainer -   test loss: 0.966809\n",
      "epoch 13 iter 17: train loss 1.04535. lr 5.057497e-04: 100%|██████████| 18/18 [00:25<00:00,  1.44s/it]\n",
      "08/22/2020 03:48:36 - INFO - mingpt.trainer -   test loss: 0.941026\n",
      "epoch 14 iter 17: train loss 1.01579. lr 4.916238e-04: 100%|██████████| 18/18 [00:22<00:00,  1.23s/it]\n",
      "08/22/2020 03:48:59 - INFO - mingpt.trainer -   test loss: 0.937161\n",
      "epoch 15 iter 17: train loss 1.00402. lr 4.767405e-04: 100%|██████████| 18/18 [00:18<00:00,  1.01s/it]\n",
      "08/22/2020 03:49:18 - INFO - mingpt.trainer -   test loss: 0.906601\n",
      "epoch 16 iter 17: train loss 0.99696. lr 4.611586e-04: 100%|██████████| 18/18 [00:18<00:00,  1.05s/it]\n",
      "08/22/2020 03:49:38 - INFO - mingpt.trainer -   test loss: 0.928030\n",
      "epoch 17 iter 17: train loss 0.96114. lr 4.449397e-04: 100%|██████████| 18/18 [00:18<00:00,  1.04s/it]\n",
      "08/22/2020 03:49:57 - INFO - mingpt.trainer -   test loss: 0.899684\n",
      "epoch 18 iter 17: train loss 0.96163. lr 4.281479e-04: 100%|██████████| 18/18 [00:20<00:00,  1.16s/it]\n",
      "08/22/2020 03:50:19 - INFO - mingpt.trainer -   test loss: 0.880867\n",
      "epoch 19 iter 17: train loss 0.96661. lr 4.108497e-04: 100%|██████████| 18/18 [00:22<00:00,  1.27s/it]\n",
      "08/22/2020 03:50:43 - INFO - mingpt.trainer -   test loss: 0.856467\n",
      "epoch 20 iter 17: train loss 0.96772. lr 3.931133e-04: 100%|██████████| 18/18 [00:30<00:00,  1.67s/it]\n",
      "08/22/2020 03:51:14 - INFO - mingpt.trainer -   test loss: 0.848194\n",
      "epoch 21 iter 17: train loss 0.92102. lr 3.750088e-04: 100%|██████████| 18/18 [00:20<00:00,  1.15s/it]\n",
      "08/22/2020 03:51:36 - INFO - mingpt.trainer -   test loss: 0.832896\n",
      "epoch 22 iter 17: train loss 0.91858. lr 3.566079e-04: 100%|██████████| 18/18 [00:27<00:00,  1.55s/it]\n",
      "08/22/2020 03:52:05 - INFO - mingpt.trainer -   test loss: 0.826457\n",
      "epoch 23 iter 17: train loss 0.91504. lr 3.379832e-04: 100%|██████████| 18/18 [00:21<00:00,  1.20s/it]\n",
      "08/22/2020 03:52:28 - INFO - mingpt.trainer -   test loss: 0.823385\n",
      "epoch 24 iter 17: train loss 0.88807. lr 3.192084e-04: 100%|██████████| 18/18 [00:28<00:00,  1.60s/it]\n",
      "08/22/2020 03:52:57 - INFO - mingpt.trainer -   test loss: 0.794581\n",
      "epoch 25 iter 17: train loss 0.88988. lr 3.003577e-04: 100%|██████████| 18/18 [00:20<00:00,  1.14s/it]\n",
      "08/22/2020 03:53:19 - INFO - mingpt.trainer -   test loss: 0.788246\n",
      "epoch 26 iter 17: train loss 0.85232. lr 2.815056e-04: 100%|██████████| 18/18 [00:19<00:00,  1.10s/it]\n",
      "08/22/2020 03:53:39 - INFO - mingpt.trainer -   test loss: 0.802030\n",
      "epoch 27 iter 17: train loss 0.86966. lr 2.627266e-04: 100%|██████████| 18/18 [00:15<00:00,  1.19it/s]\n",
      "08/22/2020 03:53:55 - INFO - mingpt.trainer -   test loss: 0.747319\n",
      "epoch 28 iter 17: train loss 0.86915. lr 2.440948e-04: 100%|██████████| 18/18 [00:17<00:00,  1.03it/s]\n",
      "08/22/2020 03:54:13 - INFO - mingpt.trainer -   test loss: 0.740153\n",
      "epoch 29 iter 17: train loss 0.84052. lr 2.256841e-04: 100%|██████████| 18/18 [00:14<00:00,  1.25it/s]\n",
      "08/22/2020 03:54:28 - INFO - mingpt.trainer -   test loss: 0.724940\n",
      "epoch 30 iter 17: train loss 0.83612. lr 2.075671e-04: 100%|██████████| 18/18 [00:16<00:00,  1.07it/s]\n",
      "08/22/2020 03:54:46 - INFO - mingpt.trainer -   test loss: 0.712849\n",
      "epoch 31 iter 17: train loss 0.81008. lr 1.898155e-04: 100%|██████████| 18/18 [00:16<00:00,  1.10it/s]\n",
      "08/22/2020 03:55:03 - INFO - mingpt.trainer -   test loss: 0.713205\n",
      "epoch 32 iter 17: train loss 0.81207. lr 1.724993e-04: 100%|██████████| 18/18 [00:17<00:00,  1.04it/s]\n",
      "08/22/2020 03:55:21 - INFO - mingpt.trainer -   test loss: 0.688224\n",
      "epoch 33 iter 17: train loss 0.80754. lr 1.556871e-04: 100%|██████████| 18/18 [00:13<00:00,  1.30it/s]\n",
      "08/22/2020 03:55:36 - INFO - mingpt.trainer -   test loss: 0.683417\n",
      "epoch 34 iter 17: train loss 0.78935. lr 1.394453e-04: 100%|██████████| 18/18 [00:13<00:00,  1.29it/s]\n",
      "08/22/2020 03:55:50 - INFO - mingpt.trainer -   test loss: 0.676795\n",
      "epoch 35 iter 17: train loss 0.80173. lr 1.238381e-04: 100%|██████████| 18/18 [00:16<00:00,  1.10it/s]\n",
      "08/22/2020 03:56:07 - INFO - mingpt.trainer -   test loss: 0.662700\n",
      "epoch 36 iter 17: train loss 0.80319. lr 1.089272e-04: 100%|██████████| 18/18 [00:12<00:00,  1.42it/s]\n",
      "08/22/2020 03:56:21 - INFO - mingpt.trainer -   test loss: 0.657306\n",
      "epoch 37 iter 17: train loss 0.82897. lr 9.477150e-05: 100%|██████████| 18/18 [00:15<00:00,  1.19it/s]\n",
      "08/22/2020 03:56:37 - INFO - mingpt.trainer -   test loss: 0.655397\n",
      "epoch 38 iter 17: train loss 0.79587. lr 8.142699e-05: 100%|██████████| 18/18 [00:16<00:00,  1.08it/s]\n",
      "08/22/2020 03:56:54 - INFO - mingpt.trainer -   test loss: 0.643176\n",
      "epoch 39 iter 17: train loss 0.78988. lr 6.894639e-05: 100%|██████████| 18/18 [00:17<00:00,  1.05it/s]\n",
      "08/22/2020 03:57:12 - INFO - mingpt.trainer -   test loss: 0.628515\n",
      "epoch 40 iter 17: train loss 0.78386. lr 6.000000e-05: 100%|██████████| 18/18 [00:16<00:00,  1.09it/s]\n",
      "08/22/2020 03:57:29 - INFO - mingpt.trainer -   test loss: 0.635055\n",
      "epoch 41 iter 17: train loss 0.78653. lr 6.000000e-05: 100%|██████████| 18/18 [00:15<00:00,  1.13it/s]\n",
      "08/22/2020 03:57:46 - INFO - mingpt.trainer -   test loss: 0.622943\n",
      "epoch 42 iter 17: train loss 0.76538. lr 6.000000e-05: 100%|██████████| 18/18 [00:17<00:00,  1.06it/s]\n",
      "08/22/2020 03:58:04 - INFO - mingpt.trainer -   test loss: 0.614995\n",
      "epoch 43 iter 17: train loss 0.77697. lr 6.000000e-05: 100%|██████████| 18/18 [00:18<00:00,  1.01s/it]\n",
      "08/22/2020 03:58:23 - INFO - mingpt.trainer -   test loss: 0.609129\n",
      "epoch 44 iter 17: train loss 0.77282. lr 6.000000e-05: 100%|██████████| 18/18 [00:13<00:00,  1.30it/s]\n",
      "08/22/2020 03:58:37 - INFO - mingpt.trainer -   test loss: 0.605170\n",
      "epoch 45 iter 17: train loss 0.77271. lr 6.000000e-05: 100%|██████████| 18/18 [00:13<00:00,  1.38it/s]\n",
      "08/22/2020 03:58:51 - INFO - mingpt.trainer -   test loss: 0.610234\n",
      "epoch 46 iter 17: train loss 0.77593. lr 6.000000e-05: 100%|██████████| 18/18 [00:13<00:00,  1.32it/s]\n",
      "08/22/2020 03:59:05 - INFO - mingpt.trainer -   test loss: 0.606665\n",
      "epoch 47 iter 17: train loss 0.74791. lr 6.000000e-05: 100%|██████████| 18/18 [00:14<00:00,  1.23it/s]\n",
      "08/22/2020 03:59:20 - INFO - mingpt.trainer -   test loss: 0.604452\n",
      "epoch 48 iter 17: train loss 0.73717. lr 6.000000e-05: 100%|██████████| 18/18 [00:15<00:00,  1.18it/s]\n",
      "08/22/2020 03:59:36 - INFO - mingpt.trainer -   test loss: 0.605354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 49 iter 17: train loss 0.74242. lr 6.000000e-05: 100%|██████████| 18/18 [00:16<00:00,  1.12it/s]\n",
      "08/22/2020 03:59:53 - INFO - mingpt.trainer -   test loss: 0.585240\n",
      "epoch 50 iter 17: train loss 0.75502. lr 6.000000e-05: 100%|██████████| 18/18 [00:17<00:00,  1.01it/s]\n",
      "08/22/2020 04:00:12 - INFO - mingpt.trainer -   test loss: 0.579192\n",
      "epoch 51 iter 17: train loss 0.75266. lr 6.000000e-05: 100%|██████████| 18/18 [00:16<00:00,  1.09it/s]\n",
      "08/22/2020 04:00:30 - INFO - mingpt.trainer -   test loss: 0.580730\n",
      "epoch 52 iter 17: train loss 0.76075. lr 6.000000e-05: 100%|██████████| 18/18 [00:17<00:00,  1.03it/s]\n",
      "08/22/2020 04:00:48 - INFO - mingpt.trainer -   test loss: 0.570095\n",
      "epoch 53 iter 17: train loss 0.74971. lr 6.000000e-05: 100%|██████████| 18/18 [00:12<00:00,  1.41it/s]\n",
      "08/22/2020 04:01:01 - INFO - mingpt.trainer -   test loss: 0.561761\n",
      "epoch 54 iter 17: train loss 0.71352. lr 6.000000e-05: 100%|██████████| 18/18 [00:14<00:00,  1.25it/s]\n",
      "08/22/2020 04:01:16 - INFO - mingpt.trainer -   test loss: 0.550329\n",
      "epoch 55 iter 17: train loss 0.72256. lr 6.000000e-05: 100%|██████████| 18/18 [00:14<00:00,  1.24it/s]\n",
      "08/22/2020 04:01:32 - INFO - mingpt.trainer -   test loss: 0.554607\n",
      "epoch 56 iter 17: train loss 0.72067. lr 6.000000e-05: 100%|██████████| 18/18 [00:17<00:00,  1.06it/s]\n",
      "08/22/2020 04:01:49 - INFO - mingpt.trainer -   test loss: 0.544566\n",
      "epoch 57 iter 17: train loss 0.71215. lr 6.000000e-05: 100%|██████████| 18/18 [00:14<00:00,  1.20it/s]\n",
      "08/22/2020 04:02:05 - INFO - mingpt.trainer -   test loss: 0.542688\n",
      "epoch 58 iter 17: train loss 0.70514. lr 6.000000e-05: 100%|██████████| 18/18 [00:13<00:00,  1.32it/s]\n",
      "08/22/2020 04:02:19 - INFO - mingpt.trainer -   test loss: 0.519356\n",
      "epoch 59 iter 17: train loss 0.70102. lr 6.000000e-05: 100%|██████████| 18/18 [00:14<00:00,  1.27it/s]\n",
      "08/22/2020 04:02:34 - INFO - mingpt.trainer -   test loss: 0.516537\n",
      "epoch 60 iter 17: train loss 0.70056. lr 6.000000e-05: 100%|██████████| 18/18 [00:14<00:00,  1.27it/s]\n",
      "08/22/2020 04:02:49 - INFO - mingpt.trainer -   test loss: 0.502580\n",
      "epoch 61 iter 17: train loss 0.68745. lr 6.894639e-05: 100%|██████████| 18/18 [00:15<00:00,  1.18it/s]\n",
      "08/22/2020 04:03:05 - INFO - mingpt.trainer -   test loss: 0.495171\n",
      "epoch 62 iter 17: train loss 0.67413. lr 8.142699e-05: 100%|██████████| 18/18 [00:14<00:00,  1.22it/s]\n",
      "08/22/2020 04:03:20 - INFO - mingpt.trainer -   test loss: 0.510591\n",
      "epoch 63 iter 17: train loss 0.70893. lr 9.477150e-05: 100%|██████████| 18/18 [00:16<00:00,  1.07it/s]\n",
      "08/22/2020 04:03:38 - INFO - mingpt.trainer -   test loss: 0.494422\n",
      "epoch 64 iter 17: train loss 0.63828. lr 1.089272e-04: 100%|██████████| 18/18 [00:14<00:00,  1.20it/s]\n",
      "08/22/2020 04:03:54 - INFO - mingpt.trainer -   test loss: 0.465332\n",
      "epoch 65 iter 17: train loss 0.65095. lr 1.238381e-04: 100%|██████████| 18/18 [00:13<00:00,  1.36it/s]\n",
      "08/22/2020 04:04:07 - INFO - mingpt.trainer -   test loss: 0.443459\n",
      "epoch 66 iter 17: train loss 0.63276. lr 1.394453e-04: 100%|██████████| 18/18 [00:12<00:00,  1.50it/s]\n",
      "08/22/2020 04:04:20 - INFO - mingpt.trainer -   test loss: 0.428796\n",
      "epoch 67 iter 17: train loss 0.59797. lr 1.556871e-04: 100%|██████████| 18/18 [00:12<00:00,  1.46it/s]\n",
      "08/22/2020 04:04:33 - INFO - mingpt.trainer -   test loss: 0.387228\n",
      "epoch 68 iter 17: train loss 0.60710. lr 1.724993e-04: 100%|██████████| 18/18 [00:14<00:00,  1.28it/s]\n",
      "08/22/2020 04:04:48 - INFO - mingpt.trainer -   test loss: 0.372774\n",
      "epoch 69 iter 17: train loss 0.58013. lr 1.898155e-04: 100%|██████████| 18/18 [00:14<00:00,  1.25it/s]\n",
      "08/22/2020 04:05:03 - INFO - mingpt.trainer -   test loss: 0.335720\n",
      "epoch 70 iter 17: train loss 0.51628. lr 2.075671e-04: 100%|██████████| 18/18 [00:13<00:00,  1.35it/s]\n",
      "08/22/2020 04:05:17 - INFO - mingpt.trainer -   test loss: 0.304998\n",
      "epoch 71 iter 17: train loss 0.50740. lr 2.256841e-04: 100%|██████████| 18/18 [00:12<00:00,  1.48it/s]\n",
      "08/22/2020 04:05:29 - INFO - mingpt.trainer -   test loss: 0.271799\n",
      "epoch 72 iter 17: train loss 0.50302. lr 2.440948e-04: 100%|██████████| 18/18 [00:16<00:00,  1.12it/s]\n",
      "08/22/2020 04:05:46 - INFO - mingpt.trainer -   test loss: 0.230124\n",
      "epoch 73 iter 17: train loss 0.40659. lr 2.627266e-04: 100%|██████████| 18/18 [00:18<00:00,  1.05s/it]\n",
      "08/22/2020 04:06:06 - INFO - mingpt.trainer -   test loss: 0.192030\n",
      "epoch 74 iter 17: train loss 0.43082. lr 2.815056e-04: 100%|██████████| 18/18 [00:15<00:00,  1.19it/s]\n",
      "08/22/2020 04:06:21 - INFO - mingpt.trainer -   test loss: 0.193672\n",
      "epoch 75 iter 17: train loss 0.42837. lr 3.003577e-04: 100%|██████████| 18/18 [00:14<00:00,  1.26it/s]\n",
      "08/22/2020 04:06:36 - INFO - mingpt.trainer -   test loss: 0.153380\n",
      "epoch 76 iter 17: train loss 0.37537. lr 3.192084e-04: 100%|██████████| 18/18 [00:15<00:00,  1.13it/s]\n",
      "08/22/2020 04:06:53 - INFO - mingpt.trainer -   test loss: 0.143604\n",
      "epoch 77 iter 17: train loss 0.36122. lr 3.379832e-04: 100%|██████████| 18/18 [00:16<00:00,  1.08it/s]\n",
      "08/22/2020 04:07:11 - INFO - mingpt.trainer -   test loss: 0.142829\n",
      "epoch 78 iter 17: train loss 0.39507. lr 3.566079e-04: 100%|██████████| 18/18 [00:15<00:00,  1.17it/s]\n",
      "08/22/2020 04:07:27 - INFO - mingpt.trainer -   test loss: 0.142977\n",
      "epoch 79 iter 17: train loss 0.35483. lr 3.750088e-04: 100%|██████████| 18/18 [00:15<00:00,  1.16it/s]\n",
      "08/22/2020 04:07:43 - INFO - mingpt.trainer -   test loss: 0.101786\n",
      "epoch 80 iter 17: train loss 0.33520. lr 3.931133e-04: 100%|██████████| 18/18 [00:15<00:00,  1.18it/s]\n",
      "08/22/2020 04:07:59 - INFO - mingpt.trainer -   test loss: 0.089523\n",
      "epoch 81 iter 17: train loss 0.33690. lr 4.108497e-04: 100%|██████████| 18/18 [00:14<00:00,  1.24it/s]\n",
      "08/22/2020 04:08:14 - INFO - mingpt.trainer -   test loss: 0.079416\n",
      "epoch 82 iter 17: train loss 0.29900. lr 4.281479e-04: 100%|██████████| 18/18 [00:15<00:00,  1.19it/s]\n",
      "08/22/2020 04:08:30 - INFO - mingpt.trainer -   test loss: 0.069599\n",
      "epoch 83 iter 17: train loss 0.37919. lr 4.449397e-04: 100%|██████████| 18/18 [00:17<00:00,  1.04it/s]\n",
      "08/22/2020 04:08:49 - INFO - mingpt.trainer -   test loss: 0.063658\n",
      "epoch 84 iter 17: train loss 0.29161. lr 4.611586e-04: 100%|██████████| 18/18 [00:16<00:00,  1.12it/s]\n",
      "08/22/2020 04:09:05 - INFO - mingpt.trainer -   test loss: 0.076999\n",
      "epoch 85 iter 17: train loss 0.29118. lr 4.767405e-04: 100%|██████████| 18/18 [00:14<00:00,  1.20it/s]\n",
      "08/22/2020 04:09:21 - INFO - mingpt.trainer -   test loss: 0.078962\n",
      "epoch 86 iter 17: train loss 0.26350. lr 4.916238e-04: 100%|██████████| 18/18 [00:16<00:00,  1.10it/s]\n",
      "08/22/2020 04:09:38 - INFO - mingpt.trainer -   test loss: 0.059769\n",
      "epoch 87 iter 17: train loss 0.27348. lr 5.057497e-04: 100%|██████████| 18/18 [00:16<00:00,  1.07it/s]\n",
      "08/22/2020 04:09:56 - INFO - mingpt.trainer -   test loss: 0.057953\n",
      "epoch 88 iter 17: train loss 0.24829. lr 5.190624e-04: 100%|██████████| 18/18 [00:16<00:00,  1.07it/s]\n",
      "08/22/2020 04:10:13 - INFO - mingpt.trainer -   test loss: 0.045550\n",
      "epoch 89 iter 17: train loss 0.27630. lr 5.315093e-04: 100%|██████████| 18/18 [00:21<00:00,  1.17s/it]\n",
      "08/22/2020 04:10:35 - INFO - mingpt.trainer -   test loss: 0.047836\n",
      "epoch 90 iter 17: train loss 0.22561. lr 5.430411e-04: 100%|██████████| 18/18 [00:14<00:00,  1.20it/s]\n",
      "08/22/2020 04:10:51 - INFO - mingpt.trainer -   test loss: 0.051563\n",
      "epoch 91 iter 17: train loss 0.26337. lr 5.536122e-04: 100%|██████████| 18/18 [00:14<00:00,  1.23it/s]\n",
      "08/22/2020 04:11:06 - INFO - mingpt.trainer -   test loss: 0.032213\n",
      "epoch 92 iter 17: train loss 0.23500. lr 5.631810e-04: 100%|██████████| 18/18 [00:13<00:00,  1.34it/s]\n",
      "08/22/2020 04:11:20 - INFO - mingpt.trainer -   test loss: 0.073665\n",
      "epoch 93 iter 17: train loss 0.28301. lr 5.717095e-04: 100%|██████████| 18/18 [00:13<00:00,  1.36it/s]\n",
      "08/22/2020 04:11:34 - INFO - mingpt.trainer -   test loss: 0.167537\n",
      "epoch 94 iter 17: train loss 0.31765. lr 5.791641e-04: 100%|██████████| 18/18 [00:12<00:00,  1.43it/s]\n",
      "08/22/2020 04:11:47 - INFO - mingpt.trainer -   test loss: 0.055927\n",
      "epoch 95 iter 17: train loss 0.24597. lr 5.855153e-04: 100%|██████████| 18/18 [00:12<00:00,  1.48it/s]\n",
      "08/22/2020 04:12:00 - INFO - mingpt.trainer -   test loss: 0.039282\n",
      "epoch 96 iter 17: train loss 0.17727. lr 5.907379e-04: 100%|██████████| 18/18 [00:12<00:00,  1.41it/s]\n",
      "08/22/2020 04:12:13 - INFO - mingpt.trainer -   test loss: 0.031256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 97 iter 17: train loss 0.14973. lr 5.948114e-04: 100%|██████████| 18/18 [00:12<00:00,  1.38it/s]\n",
      "08/22/2020 04:12:27 - INFO - mingpt.trainer -   test loss: 0.025640\n",
      "epoch 98 iter 17: train loss 0.30458. lr 5.977197e-04: 100%|██████████| 18/18 [00:13<00:00,  1.36it/s]\n",
      "08/22/2020 04:12:41 - INFO - mingpt.trainer -   test loss: 0.034791\n",
      "epoch 99 iter 17: train loss 0.18942. lr 5.994512e-04: 100%|██████████| 18/18 [00:13<00:00,  1.38it/s]\n",
      "08/22/2020 04:12:54 - INFO - mingpt.trainer -   test loss: 0.038843\n",
      "epoch 100 iter 17: train loss 0.18235. lr 5.999991e-04: 100%|██████████| 18/18 [00:12<00:00,  1.40it/s]\n",
      "08/22/2020 04:13:08 - INFO - mingpt.trainer -   test loss: 0.020059\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=100, batch_size=512, learning_rate=6e-4, \n",
    "                      weight_decay=0, betas=(0.9, 0.98), grad_norm_clip=0.,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's give the trained model an addition exam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import sample\n",
    "\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1):\n",
    "    \n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        d1d2 = x[:, :ndigit*2]\n",
    "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
    "        d3 = d1d2d3[:, -(ndigit+1):]\n",
    "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n",
    "        # decode the integers from individual digits\n",
    "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
    "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
    "        d3i_pred = (d3 * factors).sum(1)\n",
    "        d3i_gt = d1i + d2i\n",
    "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
    "            if not correct[i]:\n",
    "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
    "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that 002 + 002 = 014 (gt is 004; NOPE)\n",
      "GPT claims that 030 + 034 = 074 (gt is 064; NOPE)\n",
      "GPT claims that 002 + 001 = 013 (gt is 003; NOPE)\n",
      "GPT claims that 002 + 003 = 015 (gt is 005; NOPE)\n",
      "GPT claims that 032 + 032 = 074 (gt is 064; NOPE)\n",
      "GPT claims that 028 + 071 = 109 (gt is 099; NOPE)\n",
      "GPT claims that 030 + 033 = 073 (gt is 063; NOPE)\n",
      "GPT claims that 030 + 032 = 072 (gt is 062; NOPE)\n",
      "GPT claims that 001 + 001 = 012 (gt is 002; NOPE)\n",
      "GPT claims that 007 + 002 = 019 (gt is 009; NOPE)\n",
      "GPT claims that 000 + 001 = 011 (gt is 001; NOPE)\n",
      "GPT claims that 006 + 003 = 019 (gt is 009; NOPE)\n",
      "GPT claims that 003 + 002 = 015 (gt is 005; NOPE)\n",
      "GPT claims that 004 + 001 = 015 (gt is 005; NOPE)\n",
      "GPT claims that 004 + 002 = 016 (gt is 006; NOPE)\n",
      "GPT claims that 008 + 011 = 029 (gt is 019; NOPE)\n",
      "GPT claims that 002 + 000 = 012 (gt is 002; NOPE)\n",
      "GPT claims that 003 + 001 = 014 (gt is 004; NOPE)\n",
      "GPT claims that 000 + 002 = 012 (gt is 002; NOPE)\n",
      "GPT claims that 001 + 002 = 013 (gt is 003; NOPE)\n",
      "GPT claims that 000 + 003 = 013 (gt is 003; NOPE)\n",
      "GPT claims that 001 + 003 = 014 (gt is 004; NOPE)\n",
      "GPT claims that 008 + 001 = 019 (gt is 009; NOPE)\n",
      "final score: 8977/9000 = 99.74% correct\n"
     ]
    }
   ],
   "source": [
    "# training set: how well did we memorize?\n",
    "give_exam(train_dataset, batch_size=1024, max_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that 049 + 040 = 099 (gt is 089; NOPE)\n",
      "GPT claims that 030 + 035 = 075 (gt is 065; NOPE)\n",
      "GPT claims that 001 + 000 = 011 (gt is 001; NOPE)\n",
      "final score: 997/1000 = 99.70% correct\n"
     ]
    }
   ],
   "source": [
    "# test set: how well did we generalize?\n",
    "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that's amusing... our model learned everything except 55 + 45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
